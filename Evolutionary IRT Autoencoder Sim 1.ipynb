{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d7edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import os\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "import copy\n",
    "import random\n",
    "from operator import attrgetter\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# Evolutionary Settings\n",
    "\n",
    "POPULATION_SIZE = 30\n",
    "MUTATION_RATE = 0.30\n",
    "GENERATIONS = 10\n",
    "ELITISM = 0.80\n",
    "NUMBER_CHILDS = 8 \n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# Identifiability restriction\n",
    "\n",
    "def a_reg(a):\n",
    "    b = a * K.cast(K.greater_equal(a, 0), K.floatx())\n",
    "    c = b + K.cast(K.equal(b, 0), K.floatx())/10\n",
    "    return K.exp(K.log(c)-1/nit*K.sum(K.log(c)))\n",
    " \n",
    "def b_reg(b):\n",
    "    return b-K.sum(b)/nit;   \n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var, n_latent = args\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(tf.keras.backend.shape(z_mean)[0], n_latent))\n",
    "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# Autoencoder Estimation\n",
    "\n",
    "def AEIRT(n_layers, vector_neurons, n_items, data, epochs, batch, restriction, n_latent = 1):\n",
    "\n",
    "    inputs = Input(shape=(n_items,))\n",
    "    \n",
    "    if restriction == 'VAE':\n",
    "        if(n_layers == 0):\n",
    "            z_mean = Dense(n_latent, activation = 'linear')(inputs)\n",
    "            z_log_var = Dense(n_latent, activation = 'linear')(inputs)\n",
    "        else:\n",
    "            x1 = Dense(vector_neurons[0], activation = 'relu')(inputs)\n",
    "            for i in range(n_layers-1):\n",
    "                x1 = Dense(vector_neurons[i+1], activation = 'relu')(x1)\n",
    "            z_mean = Dense(n_latent, activation = 'linear')(x1)\n",
    "            z_log_var = Dense(n_latent, activation = 'linear')(x1)    \n",
    "\n",
    "        z = Lambda(sampling)([z_mean, z_log_var, n_latent])\n",
    "    \n",
    "        decoder_inputs = Input(shape=(n_latent,))\n",
    "\n",
    "        outputs = Dense(n_items, \n",
    "                        activation='sigmoid',\n",
    "#                        kernel_regularizer=tf.keras.regularizers.L1(0.01),\n",
    "                        kernel_initializer=initializers.Ones(),\n",
    "                        bias_initializer=initializers.Zeros())(decoder_inputs)\n",
    "        encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "        decoder = Model(decoder_inputs, outputs, name='decoder')\n",
    "\n",
    "        outputs = decoder(z)\n",
    "        vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "    \n",
    "        reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "        reconstruction_loss *= n_items\n",
    "        kl_loss = 1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var)\n",
    "        kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
    "        vae.add_loss(vae_loss)\n",
    "    \n",
    "        vae.compile(optimizer='adam')\n",
    "    \n",
    "        history = vae.fit(data,\n",
    "                            epochs=epochs,\n",
    "                            shuffle = True, \n",
    "                            batch_size=batch,\n",
    "                            validation_split = 0.2,\n",
    "                            verbose = 0,callbacks=[callback])\n",
    "    \n",
    "        latent = encoder.predict(data, verbose = 0)[0].flatten()\n",
    "    \n",
    "    else: \n",
    "        if(n_layers == 0):\n",
    "            encoded = Dense(units=1, activation='linear')(inputs)\n",
    "        else:\n",
    "            x1 = Dense(vector_neurons[0], activation = 'relu')(inputs)\n",
    "            for i in range(n_layers-1):\n",
    "                x1 = Dense(vector_neurons[i+1], activation = 'relu')(x1)\n",
    "            encoded = Dense(units=1, activation='linear')(x1)    \n",
    "    \n",
    "        decoder_inputs = Input(shape=(1,))\n",
    "    \n",
    "        # Op 1\n",
    "        if restriction == 'Product':\n",
    "            outputs = Dense(n_items, activation='sigmoid', \n",
    "                        bias_constraint=b_reg,\n",
    "                        kernel_constraint=a_reg,\n",
    "                        kernel_initializer=initializers.Ones(),\n",
    "                        bias_initializer=initializers.Zeros())(decoder_inputs)\n",
    "            encoder = Model(inputs, encoded, name='encoder')\n",
    "            decoder = Model(decoder_inputs, outputs, name='decoder')\n",
    "\n",
    "        elif restriction == 'Last Item':\n",
    "            outputs = Dense(n_items-1, activation='sigmoid',\n",
    "                        kernel_initializer=initializers.Ones(),\n",
    "                        bias_initializer=initializers.Zeros())(decoder_inputs)\n",
    "            outputs2 = Dense(units=1, activation='sigmoid',\n",
    "                       kernel_initializer= keras.initializers.Ones(),\n",
    "                       bias_initializer = keras.initializers.Zeros(),\n",
    "                       trainable = False)(decoder_inputs)\n",
    "    \n",
    "            combined_input = keras.layers.concatenate([outputs, outputs2])\n",
    "\n",
    "            encoder = Model(inputs, encoded, name='encoder')\n",
    "            decoder = Model(decoder_inputs, combined_input , name='decoder')\n",
    "    \n",
    "        else:\n",
    "            print('Restriction not found')\n",
    "            return None, None, None\n",
    "\n",
    "        outputs = decoder(encoded)\n",
    "        vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "        vae.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    \n",
    "        history = vae.fit(data,data,\n",
    "                    epochs=epochs,\n",
    "                    shuffle = True, \n",
    "                    batch_size=batch,\n",
    "                    validation_split = 0.2,\n",
    "                    verbose = 0,callbacks=[callback])\n",
    "    \n",
    "        latent = encoder.predict(data, verbose = 0).flatten()\n",
    "    \n",
    "    return history, decoder, latent\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# Evolutionary algorithm\n",
    "\n",
    "\n",
    "class Individual:\n",
    "    def __init__(self, n_layers, vector_neurons):\n",
    "        self.n_layers = n_layers\n",
    "        self.vector_neurons = vector_neurons\n",
    "        self.fitness = None\n",
    "        self.discr = None\n",
    "        self.diff = None\n",
    "        self.latent = None\n",
    "    \n",
    "    def evaluate_fitness(self, data, epochs, batch, restriction):               \n",
    "        history, decoder, latent = AEIRT(self.n_layers, self.vector_neurons, nit, data, epochs, batch, restriction)\n",
    "        self.fitness = history.history['val_loss'][-1]\n",
    "        self.discr = decoder.get_weights()[0]\n",
    "        self.diff = decoder.get_weights()[1]\n",
    "        self.latent = latent\n",
    "        \n",
    "def generate_individual():\n",
    "    n_layers = random.randint(0, 5)\n",
    "    vector_neurons = []\n",
    "    for i in range(n_layers):\n",
    "        aux = random.randint(2, nit*2)\n",
    "        aux = aux // 5 * 5 \n",
    "        aux += 1\n",
    "        vector_neurons.append(aux)\n",
    "    return Individual(n_layers, vector_neurons)\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "      \n",
    "    child1 = Individual(parent1.n_layers, parent1.vector_neurons)\n",
    "    child2 = Individual(parent2.n_layers, parent2.vector_neurons)\n",
    "        \n",
    "# It is randomized the changes in the vector of neurons\n",
    "    for i in range(child1.n_layers):\n",
    "        if parent1.n_layers <= i:\n",
    "            child1.vector_neurons[i] = parent2.vector_neurons[i]\n",
    "        elif parent2.n_layers <= i:\n",
    "            child1.vector_neurons[i] = parent1.vector_neurons[i]\n",
    "        else:\n",
    "            if random.random() < 0.5:  \n",
    "                child1.vector_neurons[i] = parent2.vector_neurons[i]\n",
    "            else:\n",
    "                child1.vector_neurons[i] = parent1.vector_neurons[i]\n",
    "                \n",
    "    for i in range(child2.n_layers):\n",
    "        if parent1.n_layers <= i:\n",
    "            child2.vector_neurons[i] = parent2.vector_neurons[i]\n",
    "        elif parent2.n_layers <= i:\n",
    "            child2.vector_neurons[i] = parent1.vector_neurons[i]\n",
    "        else:\n",
    "            if random.random() < 0.5:  \n",
    "                child2.vector_neurons[i] = parent2.vector_neurons[i]\n",
    "            else:\n",
    "                child2.vector_neurons[i] = parent1.vector_neurons[i]\n",
    "                \n",
    "    return child1, child2\n",
    "\n",
    "def mutate(individual):\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        individual.n_layers += random.randint(-1, 1)\n",
    "        if individual.n_layers < 0:\n",
    "            individual.n_layers = 0\n",
    "        if individual.n_layers > 5:\n",
    "            individual.n_layers = 5\n",
    "                \n",
    "        if individual.n_layers > len(individual.vector_neurons):\n",
    "            aux = random.randint(2, nit*2)\n",
    "            aux = aux // 5 * 5 \n",
    "            individual.vector_neurons.append(aux)\n",
    "        if individual.n_layers < len(individual.vector_neurons):\n",
    "            del individual.vector_neurons[-1]\n",
    "                \n",
    "    for i in range(len(individual.vector_neurons)):\n",
    "        if random.random() < 0.5:\n",
    "            aux = random.randint(-10, 10)\n",
    "            aux = aux // 5 * 5\n",
    "            individual.vector_neurons[i] += aux\n",
    "            if individual.vector_neurons[i] < 1:\n",
    "                individual.vector_neurons[i] = 2\n",
    "    return individual\n",
    "\n",
    "def select_parents(population, k):\n",
    "    tournament = random.sample(population, k)\n",
    "    parent1 = max(tournament, key=lambda x: x.fitness)\n",
    "    tournament.remove(parent1)\n",
    "    parent2 = max(tournament, key=lambda x: x.fitness)  \n",
    "    return parent1, parent2\n",
    "\n",
    "def evolve(population, data, epochs, batch, restriction):\n",
    "    for i in range(GENERATIONS):\n",
    "        print(f\"Generation {i+1}\")\n",
    "        population.sort(key=attrgetter('fitness'))\n",
    "        elite_count = int(ELITISM * len(population))\n",
    "        new_population = population[:elite_count]\n",
    "        for j in range(NUMBER_CHILDS):\n",
    "            if random.random() < 0.6:\n",
    "                parent1, parent2 = select_parents(population,6)\n",
    "                parent1 = copy.deepcopy(parent1)\n",
    "                parent2 = copy.deepcopy(parent2)  \n",
    "                child1, child2 = crossover(parent1, parent2)\n",
    "            else:\n",
    "                child1 = generate_individual()\n",
    "                child2 = generate_individual()\n",
    "            child1 = mutate(child1)\n",
    "            child2 = mutate(child2)\n",
    "\n",
    "            try:\n",
    "                child1.evaluate_fitness(data, epochs, batch, restriction)\n",
    "            except:\n",
    "                child1.fitness = 100\n",
    "\n",
    "\n",
    "            try:\n",
    "                child2.evaluate_fitness(data, epochs, batch, restriction)\n",
    "            except:\n",
    "                child2.fitness = 100\n",
    "                \n",
    "            new_population.append(child1)\n",
    "            new_population.append(child2)\n",
    "        population = new_population\n",
    "        population.sort(key=attrgetter('fitness'))\n",
    "        population = population[:POPULATION_SIZE]     \n",
    "\n",
    "#     Only if we want to check how the population is evolving\n",
    "#        for i in range(POPULATION_SIZE):\n",
    "#            print(f\"Layers: {population[i].n_layers} -- {population[i].vector_neurons}\")\n",
    "\n",
    "    population.sort(key=attrgetter('fitness'))\n",
    "    return population\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4eeda",
   "metadata": {},
   "source": [
    "# Product\n",
    "\n",
    "# VAE\n",
    "\n",
    "# Last Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolutionary process with the simulated data for simulation 1\n",
    "\n",
    "nit = 50\n",
    "nEx = 1000\n",
    "Npop = 1\n",
    "\n",
    "epochs=10000\n",
    "batch=64\n",
    "\n",
    "restrictionU = 'Last Item'\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "keras.utils.set_random_seed(50)\n",
    "\n",
    "\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-8,patience=50,\n",
    "                                         verbose=0,mode=\"min\",\n",
    "                                        restore_best_weights=True)\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"DataEvo/Data_1.csv\")\n",
    "\n",
    "##############################################################################################################\n",
    "# Network  \n",
    "        \n",
    "individuals = []\n",
    "for j in range(POPULATION_SIZE):\n",
    "    individual = generate_individual()\n",
    "    try:\n",
    "        individual.evaluate_fitness(data.to_numpy(), epochs, batch, restrictionU)\n",
    "    except:\n",
    "        individual.fitness = 100\n",
    "    print(individual.fitness)\n",
    "    individuals.append(individual)\n",
    "        \n",
    "pops = evolve(population = individuals,data=data.to_numpy(), epochs=epochs, batch=batch, restriction = restrictionU)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c08031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning the estimated parameters for the best-estimated networks\n",
    "\n",
    "\n",
    "parameter = pd.DataFrame()\n",
    "for i in range(POPULATION_SIZE):\n",
    "    column_name = f'column_{i}'  \n",
    "    parameter[column_name] = pops[i].discr.flatten() \n",
    "\n",
    "parameter.to_csv('Outs/aLIIC.csv')\n",
    "\n",
    "parameter = pd.DataFrame()\n",
    "for i in range(POPULATION_SIZE):\n",
    "    column_name = f'column_{i}'  \n",
    "    parameter[column_name] = pops[i].diff.flatten() \n",
    "\n",
    "parameter.to_csv('Outs/bLIIC.csv')    \n",
    "    \n",
    "\n",
    "parameter = pd.DataFrame()\n",
    "for i in range(POPULATION_SIZE):\n",
    "    column_name = f'column_{i}'  \n",
    "    parameter[column_name] = pops[i].latent.flatten() \n",
    "\n",
    "parameter.to_csv('Outs/thetasLIIC.csv')     \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92082a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_list = []\n",
    "\n",
    "for i in range(POPULATION_SIZE):\n",
    "    fitness_list.append(pops[i].fitness)\n",
    "\n",
    "\n",
    "fitness_list = pd.DataFrame(fitness_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_list.to_csv('Outs/LIfitness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(POPULATION_SIZE):\n",
    "    print(pops[i].vector_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783deea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(POPULATION_SIZE):\n",
    "    print(pops[i].fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1da03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
