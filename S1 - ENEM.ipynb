{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import os\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "import copy\n",
    "import random\n",
    "from operator import attrgetter\n",
    "from tensorflow.keras import initializers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2555c",
   "metadata": {},
   "source": [
    "# Autoencoder Function - N Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29932ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nit = 112\n",
    "epochs=10000\n",
    "batch=64\n",
    "\n",
    "data = pd.read_csv(\"Respostas.csv\")\n",
    "\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-8,patience=50,\n",
    "                                         verbose=0,mode=\"min\",\n",
    "                                        restore_best_weights=True)\n",
    "\n",
    "\n",
    "Qmat = pd.read_csv(\"Qmat.csv\")\n",
    "Qmat = Qmat.T\n",
    "Qmat = tf.constant(Qmat, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c4a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def a_reg(a):\n",
    "    a = a * K.cast(K.greater_equal(a, 0), K.floatx())\n",
    "    a = a + K.cast(K.equal(a, 0), K.floatx())/10\n",
    "    \n",
    "    nrows = a.get_shape()[0]\n",
    "    new_weights = []\n",
    "    \n",
    "    aux = Qmat.numpy()\n",
    "        \n",
    "    for j in range(nrows):\n",
    "\n",
    "        index = np.where(aux[j,:] != 0)\n",
    "        index = np.reshape(index,(np.sum(aux[j,:] != 0),))\n",
    "\n",
    "        column = a[j,:]\n",
    "\n",
    "\n",
    "        values = tf.gather(column, index)\n",
    "        t_values = tf.exp(tf.math.log(values) - 1/nit * tf.reduce_sum(tf.math.log(values)))\n",
    "        index = index.reshape(-1, 1)\n",
    "\n",
    "        new_column = tf.tensor_scatter_nd_update(column, index, t_values)\n",
    "        new_weights.append(new_column)\n",
    "    new_weights = tf.stack(new_weights, axis=0)\n",
    "    new_weights = Qmat * new_weights\n",
    "    \n",
    "    return new_weights\n",
    "\n",
    "def b_reg(b):\n",
    "   return b-K.sum(b)/nit;  \n",
    "\n",
    "\n",
    "def AEIRT(n_layers, vector_neurons, n_items, data, epochs, batch, restriction, n_latent):\n",
    "\n",
    "    inputs = Input(shape=(n_items,))\n",
    "    \n",
    "    if(n_layers == 0):\n",
    "        encoded = Dense(units=n_latent, activation='linear')(inputs)\n",
    "    else:\n",
    "        x1 = Dense(vector_neurons[0], activation = 'relu')(inputs)\n",
    "        for i in range(n_layers-1):\n",
    "            x1 = Dense(vector_neurons[i+1], activation = 'relu')(x1)\n",
    "        encoded = Dense(units=n_latent, activation='linear')(x1)    \n",
    "    \n",
    "    decoder_inputs = Input(shape=(n_latent,))\n",
    "    \n",
    "    # Op 1\n",
    "    if restriction == 'Product':\n",
    "        outputs = Dense(n_items, activation='sigmoid', \n",
    "                    bias_constraint=b_reg,\n",
    "                    kernel_constraint=a_reg,\n",
    "#                    kernel_constraint=CustomConstraint(Q_mat = Q_mat, nit = n_items),\n",
    "                    kernel_initializer=initializers.Ones(),\n",
    "                    bias_initializer=initializers.Zeros())(decoder_inputs)\n",
    "        encoder = Model(inputs, encoded, name='encoder')\n",
    "        decoder = Model(decoder_inputs, outputs, name='decoder')\n",
    "\n",
    "\n",
    "    outputs = decoder(encoded)\n",
    "    vae = Model(inputs, outputs, name='vae')\n",
    "\n",
    "    vae.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    \n",
    "    history = vae.fit(data,data,\n",
    "                epochs=epochs,\n",
    "                shuffle = True, \n",
    "                batch_size=batch,\n",
    "                validation_split = 0.2,\n",
    "                verbose = 0,callbacks=[callback])\n",
    "    \n",
    "    latent = encoder.predict(data, verbose = 0).flatten()\n",
    "    \n",
    "    return history, decoder, latent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ffb30",
   "metadata": {},
   "source": [
    "# Genetic Algorithm Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "POPULATION_SIZE = 5\n",
    "MUTATION_RATE = 0.40\n",
    "GENERATIONS = 5\n",
    "ELITISM = 0.80\n",
    "NUMBER_CHILDS = 3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262f994",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Individual:\n",
    "    def __init__(self, n_layers, vector_neurons):\n",
    "        self.n_layers = n_layers\n",
    "        self.vector_neurons = vector_neurons\n",
    "        self.fitness = None\n",
    "        self.discr = None\n",
    "        self.diff = None\n",
    "        self.latent = None\n",
    "        \n",
    "    def evaluate_fitness(self, data, epochs, batch, restriction, n_latent):               \n",
    "        history, decoder, latent = AEIRT(self.n_layers, self.vector_neurons, nit, data, epochs, batch, restriction, n_latent)\n",
    "        self.fitness = history.history['val_loss'][-1]\n",
    "        self.discr = decoder.get_weights()[0]\n",
    "        self.diff = decoder.get_weights()[1]\n",
    "        self.latent = latent\n",
    "        \n",
    "def generate_individual():\n",
    "    n_layers = random.randint(0, 5)\n",
    "    vector_neurons = []\n",
    "    for i in range(n_layers):\n",
    "        aux = random.randint(2, nit*2)\n",
    "        aux = aux // 5 * 5 \n",
    "        aux += 1\n",
    "        vector_neurons.append(aux)\n",
    "    return Individual(n_layers, vector_neurons)\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "      \n",
    "    child1 = Individual(parent1.n_layers, parent1.vector_neurons)\n",
    "    child2 = Individual(parent2.n_layers, parent2.vector_neurons)\n",
    "        \n",
    "# It is randomized the changes in the vector of neurons\n",
    "    for i in range(child1.n_layers):\n",
    "        if parent1.n_layers <= i:\n",
    "            child1.vector_neurons[i] = parent2.vector_neurons[i]\n",
    "        elif parent2.n_layers <= i:\n",
    "            child1.vector_neurons[i] = parent1.vector_neurons[i]\n",
    "        else:\n",
    "            if random.random() < 0.5:  \n",
    "                child1.vector_neurons[i] = parent2.vector_neurons[i]\n",
    "            else:\n",
    "                child1.vector_neurons[i] = parent1.vector_neurons[i]\n",
    "                \n",
    "    for i in range(child2.n_layers):\n",
    "        if parent1.n_layers <= i:\n",
    "            child2.vector_neurons[i] = parent2.vector_neurons[i]\n",
    "        elif parent2.n_layers <= i:\n",
    "            child2.vector_neurons[i] = parent1.vector_neurons[i]\n",
    "        else:\n",
    "            if random.random() < 0.5:  \n",
    "                child2.vector_neurons[i] = parent2.vector_neurons[i]\n",
    "            else:\n",
    "                child2.vector_neurons[i] = parent1.vector_neurons[i]\n",
    "                \n",
    "    return child1, child2\n",
    "\n",
    "def mutate(individual):\n",
    "    if random.random() < MUTATION_RATE:\n",
    "        individual.n_layers += random.randint(-1, 1)\n",
    "        if individual.n_layers < 0:\n",
    "            individual.n_layers = 0\n",
    "        if individual.n_layers > 5:\n",
    "            individual.n_layers = 5\n",
    "                \n",
    "        if individual.n_layers > len(individual.vector_neurons):\n",
    "            aux = random.randint(2, nit*2)\n",
    "            aux = aux // 5 * 5 \n",
    "            individual.vector_neurons.append(aux)\n",
    "        if individual.n_layers < len(individual.vector_neurons):\n",
    "            del individual.vector_neurons[-1]\n",
    "                \n",
    "    for i in range(len(individual.vector_neurons)):\n",
    "        if random.random() < 0.5:\n",
    "            aux = random.randint(-10, 10)\n",
    "            aux = aux // 5 * 5\n",
    "            individual.vector_neurons[i] += aux\n",
    "            if individual.vector_neurons[i] < 1:\n",
    "                individual.vector_neurons[i] = 2\n",
    "    return individual\n",
    "\n",
    "def select_parents(population, k):\n",
    "    tournament = random.sample(population, k)\n",
    "    parent1 = max(tournament, key=lambda x: x.fitness)\n",
    "    tournament.remove(parent1)\n",
    "    parent2 = max(tournament, key=lambda x: x.fitness)  \n",
    "    return parent1, parent2\n",
    "\n",
    "def evolve(population, data, epochs, batch, restriction, n_latent):\n",
    "    for i in range(GENERATIONS):\n",
    "        print(f\"Generation {i+1}\")\n",
    "        population.sort(key=attrgetter('fitness'))\n",
    "        elite_count = int(ELITISM * len(population))\n",
    "        new_population = population[:elite_count]\n",
    "        for j in range(NUMBER_CHILDS):\n",
    "            if random.random() < 0.6:\n",
    "                parent1, parent2 = select_parents(population,POPULATION_SIZE)\n",
    "                parent1 = copy.deepcopy(parent1)\n",
    "                parent2 = copy.deepcopy(parent2)  \n",
    "                child1, child2 = crossover(parent1, parent2)\n",
    "            else:\n",
    "                child1 = generate_individual()\n",
    "                child2 = generate_individual()\n",
    "            child1 = mutate(child1)\n",
    "            child2 = mutate(child2)\n",
    "\n",
    "            try:\n",
    "                child1.evaluate_fitness(data, epochs, batch, restriction, n_latent)\n",
    "            except:\n",
    "                child1.fitness = 100\n",
    "\n",
    "\n",
    "            try:\n",
    "                child2.evaluate_fitness(data, epochs, batch, restriction, n_latent)\n",
    "            except:\n",
    "                child2.fitness = 100\n",
    "                \n",
    "            new_population.append(child1)\n",
    "            new_population.append(child2)\n",
    "        population = new_population\n",
    "        population.sort(key=attrgetter('fitness'))\n",
    "        population = population[:POPULATION_SIZE]     \n",
    "\n",
    "#     Only if we want to check how the population is evolving\n",
    "#        for i in range(POPULATION_SIZE):\n",
    "#            print(f\"Layers: {population[i].n_layers} -- {population[i].vector_neurons}\")\n",
    "\n",
    "    population.sort(key=attrgetter('fitness'))\n",
    "    return population\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05c7c8",
   "metadata": {},
   "source": [
    "# Product Restriction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4139244",
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals = []\n",
    "for i in range(POPULATION_SIZE):\n",
    "    individual = generate_individual()\n",
    "    individual.evaluate_fitness(data.to_numpy(), epochs, batch, 'Product', 3)\n",
    "    individuals.append(individual)\n",
    "#    print(f\"Layers: {pops[i].n_layers} -- {pops[i].vector_neurons} -- Loss: \", pops[i].fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "pops = evolve(population = individuals,data=data.to_numpy(),\n",
    "              epochs=epochs, batch=batch, restriction = 'Product', n_latent = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pops)):\n",
    "    print(f\"Layers: {pops[i].n_layers} -- {pops[i].vector_neurons} -- Loss: \", pops[i].fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4056e6b",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2faa4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pops[0].discr).T.to_csv('EnemDiscr.csv')\n",
    "pd.DataFrame(pops[0].diff).to_csv('EnemDiff.csv')\n",
    "pd.DataFrame(pops[0].latent).to_csv('EnemLatent.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
